{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad79303",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.image import adjust_contrast\n",
    "from tensorflow.keras import layers, Input, models, Model, backend\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.initializers import HeNormal, Zeros\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7655c4",
   "metadata": {},
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   \n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Hyperparameters\n",
    "batch_size = 128\n",
    "epochs = 500\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69417b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Initializers\n",
    "kernel_init = HeNormal()  # He Normal initialization\n",
    "bias_init = Zeros()  # The bias is initialized to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781357c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "images = np.load('./task_2/data/data_big/images.npy')\n",
    "labels = np.load('./task_2/data/data_big/labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97b3a8",
   "metadata": {},
   "source": [
    "images = tf.squeeze(tf.image.resize(tf.expand_dims(images, -1), [299, 299]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8cec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize image data, scaling pixel values ​​to [0, 1]\n",
    "images = images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed380dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the image is grayscale, add a dimension at the end to fit Keras's input shape requirements\n",
    "if len(images.shape) == 3:\n",
    "    images = np.expand_dims(images, axis=-1)  # images = np.repeat(images, 3, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03bbf4",
   "metadata": {},
   "source": [
    "def preprocess_image(image):\n",
    "    images = tf.image.adjust_brightness(image, -0.2)  # Adjust brightness\n",
    "    images = adjust_contrast(images, contrast_factor=2.5)  # Enhance contrast\n",
    "    images = adjust_gamma(images, gain=1.0, gamma=4)\n",
    "    images = tf.clip_by_value(images, 0.0, 1.0)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7509f",
   "metadata": {},
   "source": [
    "images = preprocess_image(images).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ea23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate hours and minutes from labels\n",
    "y_train_hours = y_train[:, 0]\n",
    "y_train_minutes = y_train[:, 1]\n",
    "y_val_hours = y_val[:, 0]\n",
    "y_val_minutes = y_val[:, 1]\n",
    "y_test_hours = y_test[:, 0]\n",
    "y_test_minutes = y_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7811c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(y_train_hours, bins=12)  # Check the distribution of hour labels\n",
    "# plt.hist(y_train_minutes, bins=60)  # Check the distribution of minute labels\n",
    "# plt.show()\n",
    "# exit(0)\n",
    "def build_custom_model(input_shape):\n",
    "    # Input Layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # Convolutional and pooling layers\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    # x = layers.BatchNormalization()(x)  # add Batch Normalization\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(x)\n",
    "    # x = layers.BatchNormalization()(x)  # add Batch Normalization\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    # x = layers.BatchNormalization()(x)  # add Batch Normalization\n",
    "    # # Global Average Pooling Layer\n",
    "    # x = layers.GlobalAveragePooling2D()(x)\n",
    "    # Defining Output\n",
    "    model = models.Model(inputs=inputs, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787634e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = images[0].shape\n",
    "base_model = build_custom_model(input_shape=input_shape)\n",
    "# base_model = VGG16(weights='imagenet', include_top=False, input_shape=None, pooling='avg')\n",
    "# base_model = VGG16(weights='imagenet', include_top=False)\n",
    "# Set path to save weights\n",
    "weights_path = os.path.join(os.getcwd(), 'DIY.weights.h5')\n",
    "# Save model weights\n",
    "base_model.save_weights(weights_path)\n",
    "# Define optimizer with adjustable learning rate\n",
    "# adam = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe060874",
   "metadata": {},
   "source": [
    "# Freeze layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add decoder\n",
    "encoder = base_model.output\n",
    "decoder_1 = Flatten()(encoder)\n",
    "decoder_1 = Dense(32, activation='relu')(decoder_1)\n",
    "decoder_1 = Dropout(0.1)(decoder_1)\n",
    "decoder_hour = Dense(12, activation='sigmoid', name='hour_output')(decoder_1)\n",
    "decoder_2 = Flatten()(encoder)\n",
    "decoder_2 = Dense(256, activation='relu')(decoder_2)\n",
    "decoder_2 = Dropout(0.1)(decoder_2)\n",
    "decoder_min = Dense(60, activation='sigmoid', name='minute_output')(decoder_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086cfe6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Construct multi-head model\n",
    "model = Model(inputs=base_model.input, outputs=[decoder_hour, decoder_min])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124dd40e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73326def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_hour_loss(y_true, y_pred):\n",
    "    # Convert y_true to integer labels\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    # Find the most likely predicted class\n",
    "    pred_classes = tf.argmax(y_pred, axis=-1)\n",
    "    pred_classes = tf.cast(pred_classes, tf.float32)\n",
    "\n",
    "    # Calculate forward error and backward error (12-hour format)\n",
    "    forward_diff = tf.abs(pred_classes - y_true)\n",
    "    backward_diff = 12 - forward_diff\n",
    "\n",
    "    # Interpolate based on the minimum error, with the smaller error category receiving higher weight\n",
    "    circular_diff = tf.minimum(forward_diff, backward_diff)\n",
    "\n",
    "    # Calculate interpolated loss value\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Weight the loss with circular error\n",
    "    weighted_loss = loss * (1 - circular_diff / 12)\n",
    "\n",
    "    return backend.mean(weighted_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a6021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_minute_loss(y_true, y_pred):\n",
    "    # Convert y_true to integer labels\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    # Find the most likely predicted class\n",
    "    pred_classes = tf.argmax(y_pred, axis=-1)\n",
    "    pred_classes = tf.cast(pred_classes, tf.float32)\n",
    "\n",
    "    # Calculate forward error and backward error (12-hour format)\n",
    "    forward_diff = tf.abs(pred_classes - y_true)\n",
    "    backward_diff = 60 - forward_diff\n",
    "\n",
    "    # Interpolate based on the minimum error, with the smaller error category receiving higher weight\n",
    "    circular_diff = tf.minimum(forward_diff, backward_diff)\n",
    "\n",
    "    # If the error is within 5 minutes, consider the loss as 0\n",
    "    zero_loss_mask = tf.cast(circular_diff <= 10, tf.float32)\n",
    "\n",
    "    # Calculate interpolated loss value\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Weight the loss with circular error\n",
    "    weighted_loss = loss * (1 - zero_loss_mask / 60) * (1 - zero_loss_mask)\n",
    "\n",
    "    return backend.mean(weighted_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ffde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              # loss={'hour_output': 'sparse_categorical_crossentropy', 'minute_output': 'sparse_categorical_crossentropy'},\n",
    "              loss={'hour_output': circular_hour_loss, 'minute_output': circular_minute_loss},\n",
    "              metrics={'hour_output': 'accuracy', 'minute_output': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b182578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753fa2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation function\n",
    "def augment(images, labels):\n",
    "    images = tf.image.random_brightness(images, 0.2)  # Adjust brightness\n",
    "    images = tf.image.random_contrast(images, 1, 2.0)  # Enhance contrast\n",
    "    images = tf.image.rot90(images, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))  # Random rotation\n",
    "    return images, labels  # Return augmented images and original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tf.data.Dataset dataset and apply data augmentation\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train, {'hour_output': y_train_hours, 'minute_output': y_train_minutes}))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).map(augment).batch(batch_size).prefetch(\n",
    "    tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69054382",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "first_element = next(iter(train_dataset.unbatch()))\n",
    "import matplotlib.pyplot as plt\n",
    "debug_show_pic = first_element[0]\n",
    "# This line is to change shape, delete if unnecessary\n",
    "plt.imshow(debug_show_pic, cmap='gray')\n",
    "plt.show()\n",
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ce1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, {'hour_output': y_val_hours, 'minute_output': y_val_minutes}))\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4bea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback functions\n",
    "early_stopping = EarlyStopping(monitor='val_minute_output_accuracy', patience=20, restore_best_weights=True, mode='max')\n",
    "checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_minute_output_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f87557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and add callback\n",
    "history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=[early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classification_multihead_without_labels_train_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61849be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the total loss curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Total training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation total loss')\n",
    "plt.title('Multi-head classification total loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03888762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve for each output\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['hour_output_loss'], label='Output training loss in hours')\n",
    "plt.plot(history.history['val_hour_output_loss'], label='Hourly output validation loss')\n",
    "plt.plot(history.history['minute_output_loss'], label='Minute output training loss')\n",
    "plt.plot(history.history['val_minute_output_loss'], label='Minute output validation loss')\n",
    "plt.title('Multi-head classification output loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38524086",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247832b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy curve of hourly output\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['hour_output_accuracy'], label='Hourly output training accuracy')\n",
    "plt.plot(history.history['val_hour_output_accuracy'], label='Hourly output verification accuracy')\n",
    "plt.title('Multi-head classification hourly output accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c510360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the accuracy curve of minute output\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['minute_output_accuracy'], label='Minute output training accuracy')\n",
    "plt.plot(history.history['val_minute_output_accuracy'], label='Minute output verification accuracy')\n",
    "plt.title('Multi-head classification minute output accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe40431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "results = model.evaluate(X_test, {'hour_output': y_test_hours, 'minute_output': y_test_minutes})\n",
    "test_loss = results[0]\n",
    "test_hour_acc = results[3]\n",
    "test_minute_acc = results[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Hour Accuracy: {test_hour_acc}, Test Minute Accuracy: {test_minute_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd07c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a test image and make predictions\n",
    "sample_test_image = X_test[0:1]  # Select an image and keep its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd48bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "hour_pred, minute_pred = model.predict(sample_test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction results\n",
    "predicted_hour = np.argmax(hour_pred)\n",
    "predicted_minute = np.argmax(minute_pred)\n",
    "print(f'Predicted Hour: {predicted_hour}, Predicted Minute: {predicted_minute}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2481ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original image and the predicted results\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sample_test_image.squeeze(), cmap='gray')\n",
    "plt.title(f'Predicted Time: {predicted_hour:02}:{predicted_minute:02}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
